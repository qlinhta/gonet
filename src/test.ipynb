{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c069c49dbaf1f56a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:35:25.171339Z",
     "start_time": "2024-01-30T09:35:22.745273Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "NON_LINEARITY = {\n",
    "    'ReLU': nn.ReLU(inplace=True),\n",
    "    'Swish': Swish(),\n",
    "}\n",
    "\n",
    "def _RoundChannels(c, divisor=8, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_c = max(min_value, int(c + divisor / 2) // divisor * divisor)\n",
    "    if new_c < 0.9 * c:\n",
    "        new_c += divisor\n",
    "    return new_c\n",
    "\n",
    "def _SplitChannels(channels, num_groups):\n",
    "    split_channels = [channels//num_groups for _ in range(num_groups)]\n",
    "    split_channels[0] += channels - sum(split_channels)\n",
    "    return split_channels\n",
    "\n",
    "def Conv3x3Bn(in_channels, out_channels, stride, non_linear='ReLU'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        NON_LINEARITY[non_linear]\n",
    "    )\n",
    "\n",
    "def Conv1x1Bn(in_channels, out_channels, non_linear='ReLU'):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        NON_LINEARITY[non_linear]\n",
    "    )\n",
    "\n",
    "class SqueezeAndExcite(nn.Module):\n",
    "    def __init__(self, channels, squeeze_channels, se_ratio):\n",
    "        super(SqueezeAndExcite, self).__init__()\n",
    "\n",
    "        squeeze_channels = squeeze_channels * se_ratio\n",
    "        if not squeeze_channels.is_integer():\n",
    "            raise ValueError('channels must be divisible by 1/ratio')\n",
    "\n",
    "        squeeze_channels = int(squeeze_channels)\n",
    "        self.se_reduce = nn.Conv2d(channels, squeeze_channels, 1, 1, 0, bias=True)\n",
    "        self.non_linear1 = NON_LINEARITY['Swish']\n",
    "        self.se_expand = nn.Conv2d(squeeze_channels, channels, 1, 1, 0, bias=True)\n",
    "        self.non_linear2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.mean(x, (2, 3), keepdim=True)\n",
    "        y = self.non_linear1(self.se_reduce(y))\n",
    "        y = self.non_linear2(self.se_expand(y))\n",
    "        y = x * y\n",
    "\n",
    "        return y\n",
    "\n",
    "class GroupedConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(GroupedConv2d, self).__init__()\n",
    "\n",
    "        self.num_groups = len(kernel_size)\n",
    "        self.split_in_channels = _SplitChannels(in_channels, self.num_groups)\n",
    "        self.split_out_channels = _SplitChannels(out_channels, self.num_groups)\n",
    "\n",
    "        self.grouped_conv = nn.ModuleList()\n",
    "        for i in range(self.num_groups):\n",
    "            self.grouped_conv.append(nn.Conv2d(\n",
    "                self.split_in_channels[i],\n",
    "                self.split_out_channels[i],\n",
    "                kernel_size[i],\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "                bias=False\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_groups == 1:\n",
    "            return self.grouped_conv[0](x)\n",
    "\n",
    "        x_split = torch.split(x, self.split_in_channels, dim=1)\n",
    "        x = [conv(t) for conv, t in zip(self.grouped_conv, x_split)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MDConv(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, stride):\n",
    "        super(MDConv, self).__init__()\n",
    "\n",
    "        self.num_groups = len(kernel_size)\n",
    "        self.split_channels = _SplitChannels(channels, self.num_groups)\n",
    "\n",
    "        self.mixed_depthwise_conv = nn.ModuleList()\n",
    "        for i in range(self.num_groups):\n",
    "            self.mixed_depthwise_conv.append(nn.Conv2d(\n",
    "                self.split_channels[i],\n",
    "                self.split_channels[i],\n",
    "                kernel_size[i],\n",
    "                stride=stride,\n",
    "                padding=kernel_size[i]//2,\n",
    "                groups=self.split_channels[i],\n",
    "                bias=False\n",
    "            ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_groups == 1:\n",
    "            return self.mixed_depthwise_conv[0](x)\n",
    "\n",
    "        x_split = torch.split(x, self.split_channels, dim=1)\n",
    "        x = [conv(t) for conv, t in zip(self.mixed_depthwise_conv, x_split)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MixNetBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=[3],\n",
    "            expand_ksize=[1],\n",
    "            project_ksize=[1],\n",
    "            stride=1,\n",
    "            expand_ratio=1,\n",
    "            non_linear='ReLU',\n",
    "            se_ratio=0.0\n",
    "    ):\n",
    "\n",
    "        super(MixNetBlock, self).__init__()\n",
    "\n",
    "        expand = (expand_ratio != 1)\n",
    "        expand_channels = in_channels * expand_ratio\n",
    "        se = (se_ratio != 0.0)\n",
    "        self.residual_connection = (stride == 1 and in_channels == out_channels)\n",
    "\n",
    "        conv = []\n",
    "\n",
    "        if expand:\n",
    "            pw_expansion = nn.Sequential(\n",
    "                GroupedConv2d(in_channels, expand_channels, expand_ksize),\n",
    "                nn.BatchNorm2d(expand_channels),\n",
    "                NON_LINEARITY[non_linear]\n",
    "            )\n",
    "            conv.append(pw_expansion)\n",
    "\n",
    "        dw = nn.Sequential(\n",
    "            MDConv(expand_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(expand_channels),\n",
    "            NON_LINEARITY[non_linear]\n",
    "        )\n",
    "        conv.append(dw)\n",
    "\n",
    "        if se:\n",
    "            squeeze_excite = SqueezeAndExcite(expand_channels, in_channels, se_ratio)\n",
    "            conv.append(squeeze_excite)\n",
    "\n",
    "        pw_projection = nn.Sequential(\n",
    "            GroupedConv2d(expand_channels, out_channels, project_ksize),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        conv.append(pw_projection)\n",
    "\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual_connection:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class MixNet(nn.Module):\n",
    "    # [in_channels, out_channels, kernel_size, expand_ksize, project_ksize, stride, expand_ratio, non_linear, se_ratio]\n",
    "    mixnet_s = [(16,  16,  [3],              [1],    [1],    1, 1, 'ReLU',  0.0),\n",
    "                (16,  24,  [3],              [1, 1], [1, 1], 2, 6, 'ReLU',  0.0),\n",
    "                (24,  24,  [3],              [1, 1], [1, 1], 1, 3, 'ReLU',  0.0),\n",
    "                (24,  40,  [3, 5, 7],        [1],    [1],    2, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],           [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],           [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],           [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  80,  [3, 5, 7],        [1],    [1, 1], 2, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5],           [1],    [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5],           [1],    [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  120, [3, 5, 7],        [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9],     [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9],     [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 200, [3, 5, 7, 9, 11], [1],    [1],    2, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9],     [1],    [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9],     [1],    [1, 1], 1, 6, 'Swish', 0.5)]\n",
    "\n",
    "    mixnet_m = [(24,  24,  [3],          [1],    [1],    1, 1, 'ReLU',  0.0),\n",
    "                (24,  32,  [3, 5, 7],    [1, 1], [1, 1], 2, 6, 'ReLU',  0.0),\n",
    "                (32,  32,  [3],          [1, 1], [1, 1], 1, 3, 'ReLU',  0.0),\n",
    "                (32,  40,  [3, 5, 7, 9], [1],    [1],    2, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],       [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],       [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  40,  [3, 5],       [1, 1], [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (40,  80,  [3, 5, 7],    [1],    [1],    2, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  80,  [3, 5, 7, 9], [1, 1], [1, 1], 1, 6, 'Swish', 0.25),\n",
    "                (80,  120, [3],          [1],    [1],    1, 6, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 120, [3, 5, 7, 9], [1, 1], [1, 1], 1, 3, 'Swish', 0.5),\n",
    "                (120, 200, [3, 5, 7, 9], [1],    [1], 2, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9], [1],    [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9], [1],    [1, 1], 1, 6, 'Swish', 0.5),\n",
    "                (200, 200, [3, 5, 7, 9], [1],    [1, 1], 1, 6, 'Swish', 0.5)]\n",
    "\n",
    "    def __init__(self, net_type='mixnet_s', input_size=224, num_classes=1000, stem_channels=16, feature_size=1536, depth_multiplier=1.0):\n",
    "        super(MixNet, self).__init__()\n",
    "\n",
    "        if net_type == 'mixnet_s':\n",
    "            config = self.mixnet_s\n",
    "            stem_channels = 16\n",
    "            dropout_rate = 0.2\n",
    "        elif net_type == 'mixnet_m':\n",
    "            config = self.mixnet_m\n",
    "            stem_channels = 24\n",
    "            dropout_rate = 0.25\n",
    "        elif net_type == 'mixnet_l':\n",
    "            config = self.mixnet_m\n",
    "            stem_channels = 24\n",
    "            depth_multiplier *= 1.3\n",
    "            dropout_rate = 0.25\n",
    "        else:\n",
    "            raise TypeError('Unsupported MixNet type')\n",
    "\n",
    "        assert input_size % 32 == 0\n",
    "\n",
    "        # depth multiplier\n",
    "        if depth_multiplier != 1.0:\n",
    "            stem_channels = _RoundChannels(stem_channels*depth_multiplier)\n",
    "\n",
    "            for i, conf in enumerate(config):\n",
    "                conf_ls = list(conf)\n",
    "                conf_ls[0] = _RoundChannels(conf_ls[0]*depth_multiplier)\n",
    "                conf_ls[1] = _RoundChannels(conf_ls[1]*depth_multiplier)\n",
    "                config[i] = tuple(conf_ls)\n",
    "\n",
    "        # stem convolution\n",
    "        self.stem_conv = Conv3x3Bn(3, stem_channels, 2)\n",
    "\n",
    "        # building MixNet blocks\n",
    "        layers = []\n",
    "        for in_channels, out_channels, kernel_size, expand_ksize, project_ksize, stride, expand_ratio, non_linear, se_ratio in config:\n",
    "            layers.append(MixNetBlock(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                expand_ksize=expand_ksize,\n",
    "                project_ksize=project_ksize,\n",
    "                stride=stride,\n",
    "                expand_ratio=expand_ratio,\n",
    "                non_linear=non_linear,\n",
    "                se_ratio=se_ratio\n",
    "            ))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        # last several layers\n",
    "        self.head_conv = Conv1x1Bn(config[-1][1], feature_size)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(input_size//32, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(feature_size, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem_conv(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.head_conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:35:30.577463Z",
     "start_time": "2024-01-30T09:35:30.563457Z"
    }
   },
   "id": "648489166992ae08"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    net = MixNet()\n",
    "    x_image = Variable(torch.randn(1, 3, 224, 224))\n",
    "    y = net(x_image)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:37:32.308212Z",
     "start_time": "2024-01-30T09:37:31.673316Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T09:37:32.313826Z",
     "start_time": "2024-01-30T09:37:32.308035Z"
    }
   },
   "id": "d384b62e3601b62c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ab87e403ccca5b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
